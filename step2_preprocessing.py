# ============================================================================
# ENHANCED STEP 2: FEATURE ENGINEERING FOR MAXIMUM F1 SCORE
# Major improvements over original preprocessing
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import mutual_info_classif
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# 2.1: Load Data from Step 1
# ============================================================================
print("Loading data from Step 1...")
df = pd.read_pickle('df_after_eda.pkl')
print(f"Data loaded: {df.shape}")
print("âœ“ Data loaded!\n")

# ============================================================================
# 2.2: Create Binary Target Variable
# ============================================================================
print("="*70)
print("CREATING BINARY TARGET VARIABLE")
print("="*70)

target_map = {
    'Fully Paid': 0,
    'Charged Off': 1,
    'Default': 1,
    'Does not meet the credit policy. Status:Fully Paid': 0,
    'Does not meet the credit policy. Status:Charged Off': 1
}

print("\nTarget Mapping:")
for status, label in target_map.items():
    print(f"  {status} -> {label}")

print(f"\nOriginal dataset size: {len(df):,} rows")
df_clean = df[df['loan_status'].isin(target_map.keys())].copy()
print(f"After filtering: {len(df_clean):,} rows")

df_clean['target'] = df_clean['loan_status'].map(target_map)

print("\nTarget Distribution:")
print(f"  Fully Paid (0): {(df_clean['target']==0).sum():,} ({(df_clean['target']==0).mean()*100:.2f}%)")
print(f"  Default (1):    {(df_clean['target']==1).sum():,} ({(df_clean['target']==1).mean()*100:.2f}%)")

print("âœ“ Target variable created!\n")

# ============================================================================
# 2.3: ENHANCED FEATURE SELECTION (MORE FEATURES)
# ============================================================================
print("="*70)
print("ENHANCED FEATURE SELECTION")
print("="*70)

# CHANGE 1: Add MORE numerical features for better patterns
numerical_features = [
    # Original features
    'loan_amnt',
    'int_rate',
    'installment',
    'annual_inc',
    'dti',
    'open_acc',
    'pub_rec',
    'revol_bal',
    'revol_util',
    'total_acc',
    'mort_acc',
    'pub_rec_bankruptcies',
    # ADDED: More powerful features
    'funded_amnt',           # How much was actually funded
    'funded_amnt_inv',       # Investor funding
    'delinq_2yrs',           # Delinquencies in last 2 years - STRONG predictor
    'inq_last_6mths',        # Credit inquiries - desperation signal
    'mths_since_last_delinq', # Time since last delinquency
    'mths_since_last_record', # Time since last public record
    'collections_12_mths_ex_med', # Collections - strong default signal
    'acc_now_delinq',        # Currently delinquent accounts
    'tot_coll_amt',          # Total collection amounts
    'tot_cur_bal',           # Total current balance
    'total_rev_hi_lim',      # Total revolving credit limit
]

categorical_features = [
    'term',
    'grade',
    'sub_grade',             # ADDED: More granular than grade
    'emp_length',
    'home_ownership',
    'verification_status',
    'purpose',
    'addr_state',
    'initial_list_status',   # ADDED: Whole vs fractional loan
    'application_type',      # ADDED: Individual vs joint
]

print(f"\nEnhanced Feature Set:")
print(f"  Numerical: {len(numerical_features)} (was 12)")
print(f"  Categorical: {len(categorical_features)} (was 7)")

# Keep only features that exist
numerical_features = [f for f in numerical_features if f in df_clean.columns]
categorical_features = [f for f in categorical_features if f in df_clean.columns]

print(f"\nAfter checking availability:")
print(f"  Numerical: {len(numerical_features)}")
print(f"  Categorical: {len(categorical_features)}")

features_to_use = numerical_features + categorical_features
required_cols = features_to_use + ['target', 'loan_status']
df_features = df_clean[required_cols].copy()

print(f"\nâœ“ Feature dataframe: {df_features.shape}\n")

# ============================================================================
# 2.4: DATA CLEANING WITH BETTER MISSING VALUE STRATEGY
# ============================================================================
print("="*70)
print("ENHANCED MISSING VALUE HANDLING")
print("="*70)

# CHANGE 2: Create missing value indicators (these can be predictive!)
print("\nCreating missing value indicators...")
for col in numerical_features:
    if df_features[col].isnull().sum() > 0:
        indicator_name = f"{col}_is_missing"
        df_features[indicator_name] = df_features[col].isnull().astype(int)
        print(f"  {indicator_name}: {df_features[indicator_name].sum()} missing values tracked")

# CHANGE 3: Fill numerical with median PER TARGET CLASS (better than overall median)
print("\nFilling numerical features with class-specific median...")
for col in numerical_features:
    if df_features[col].isnull().sum() > 0:
        # Fill missing values with median of the same target class
        median_0 = df_features[df_features['target']==0][col].median()
        median_1 = df_features[df_features['target']==1][col].median()
        
        df_features.loc[(df_features['target']==0) & (df_features[col].isnull()), col] = median_0
        df_features.loc[(df_features['target']==1) & (df_features[col].isnull()), col] = median_1
        
        print(f"  {col}: Filled with median_0={median_0:.2f}, median_1={median_1:.2f}")

# Fill categorical with 'Unknown'
print("\nFilling categorical features...")
for col in categorical_features:
    if df_features[col].isnull().sum() > 0:
        df_features[col] = df_features[col].fillna('Unknown')

# Remove remaining NaN rows
rows_before = len(df_features)
df_features = df_features.dropna()
print(f"\nRemoved {rows_before - len(df_features):,} rows with remaining NaN")

print("âœ“ Missing values handled!\n")

# ============================================================================
# 2.5: SMARTER OUTLIER HANDLING
# ============================================================================
print("="*70)
print("ENHANCED OUTLIER HANDLING")
print("="*70)

# CHANGE 4: Use percentile-based capping instead of hard thresholds
print("\nCapping outliers at 99th percentile...")

outlier_caps = {
    'annual_inc': df_features['annual_inc'].quantile(0.99),
    'dti': df_features['dti'].quantile(0.99),
    'revol_util': df_features['revol_util'].quantile(0.99),
    'revol_bal': df_features['revol_bal'].quantile(0.99),
}

for col, cap_value in outlier_caps.items():
    if col in df_features.columns:
        before = (df_features[col] > cap_value).sum()
        df_features[col] = df_features[col].clip(upper=cap_value)
        print(f"  {col}: Capped {before:,} values at {cap_value:.2f}")

print(f"\nâœ“ Outliers handled!\n")

# ============================================================================
# 2.6: CREATE POWERFUL INTERACTION FEATURES
# ============================================================================
print("="*70)
print("CREATING INTERACTION FEATURES (KEY FOR F1 BOOST)")
print("="*70)

# CHANGE 5: Add domain-knowledge interaction features
print("\nCreating financial ratio features...")

# Financial health ratios
df_features['debt_to_income_ratio'] = df_features['dti'] / 100  # Normalize
df_features['payment_to_income'] = (df_features['installment'] * 12) / (df_features['annual_inc'] + 1)
df_features['loan_to_income'] = df_features['loan_amnt'] / (df_features['annual_inc'] + 1)
df_features['interest_burden'] = df_features['int_rate'] * df_features['loan_amnt'] / 100

# Credit utilization features
if 'revol_util' in df_features.columns:
    df_features['revol_util_squared'] = df_features['revol_util'] ** 2  # Non-linear effect
    df_features['high_utilization'] = (df_features['revol_util'] > 75).astype(int)

# Credit history features
df_features['avg_account_age'] = df_features['total_acc'] / (df_features['open_acc'] + 1)
df_features['credit_depth'] = df_features['total_acc'] * df_features['open_acc']

# Risk indicators
if 'pub_rec' in df_features.columns and 'pub_rec_bankruptcies' in df_features.columns:
    df_features['total_negative_marks'] = df_features['pub_rec'] + df_features['pub_rec_bankruptcies']
    df_features['has_bankruptcies'] = (df_features['pub_rec_bankruptcies'] > 0).astype(int)

if 'delinq_2yrs' in df_features.columns:
    df_features['has_recent_delinq'] = (df_features['delinq_2yrs'] > 0).astype(int)

if 'inq_last_6mths' in df_features.columns:
    df_features['high_inquiries'] = (df_features['inq_last_6mths'] > 2).astype(int)

# Combined risk score
risk_features = []
if 'delinq_2yrs' in df_features.columns:
    risk_features.append('delinq_2yrs')
if 'pub_rec' in df_features.columns:
    risk_features.append('pub_rec')
if 'pub_rec_bankruptcies' in df_features.columns:
    risk_features.append('pub_rec_bankruptcies')
if 'inq_last_6mths' in df_features.columns:
    risk_features.append('inq_last_6mths')

if risk_features:
    df_features['combined_risk_score'] = df_features[risk_features].sum(axis=1)

print(f"Created {len([c for c in df_features.columns if c not in features_to_use and c != 'target'])} interaction features")

# ============================================================================
# 2.7: CREATE BINNED FEATURES FOR NON-LINEAR PATTERNS
# ============================================================================
print("\n" + "="*70)
print("CREATING BINNED FEATURES")
print("="*70)

# CHANGE 6: Bin key continuous features to capture threshold effects
print("\nBinning key features...")

# Income brackets (thresholds matter for default)
df_features['income_bracket'] = pd.cut(
    df_features['annual_inc'],
    bins=[0, 30000, 50000, 75000, 100000, np.inf],
    labels=[0, 1, 2, 3, 4]
)
df_features['income_bracket'] = df_features['income_bracket'].cat.add_categories([-1]).fillna(-1).astype(int)

# DTI brackets
if 'dti' in df_features.columns:
    df_features['dti_bracket'] = pd.cut(
        df_features['dti'],
        bins=[0, 10, 20, 30, 40, np.inf],
        labels=[0, 1, 2, 3, 4]
    )
    df_features['dti_bracket'] = df_features['dti_bracket'].cat.add_categories([-1]).fillna(-1).astype(int)

# Interest rate brackets
df_features['rate_bracket'] = pd.cut(
    df_features['int_rate'],
    bins=[0, 8, 12, 16, 20, np.inf],
    labels=[0, 1, 2, 3, 4]
)
df_features['rate_bracket'] = df_features['rate_bracket'].cat.add_categories([-1]).fillna(-1).astype(int)

print("Created binned features: income_bracket, dti_bracket, rate_bracket")

# ============================================================================
# 2.8: ENCODE CATEGORICAL VARIABLES
# ============================================================================
print("\n" + "="*70)
print("ENCODING CATEGORICAL VARIABLES")
print("="*70)

label_encoders = {}

print("\nLabel encoding categorical features...")
for col in categorical_features:
    if col in df_features.columns:
        le = LabelEncoder()
        df_features[col] = le.fit_transform(df_features[col].astype(str))
        label_encoders[col] = le
        print(f"  {col}: Encoded {len(le.classes_)} categories")

print("âœ“ Encoding complete!\n")

# ============================================================================
# 2.9: FEATURE SELECTION - REMOVE LOW IMPORTANCE FEATURES
# ============================================================================
print("="*70)
print("FEATURE SELECTION BY MUTUAL INFORMATION")
print("="*70)

# Get all feature columns (excluding target)
all_feature_cols = [c for c in df_features.columns if c not in ['target', 'loan_status']]

# Calculate mutual information
print("\nCalculating feature importance...")
mi_scores = mutual_info_classif(df_features[all_feature_cols], df_features['target'], random_state=42)

# Create importance dataframe
importance_df = pd.DataFrame({
    'feature': all_feature_cols,
    'importance': mi_scores
}).sort_values('importance', ascending=False)

print("\nTop 20 most important features:")
print(importance_df.head(20).to_string(index=False))

# CHANGE 7: Keep only features with importance > threshold
importance_threshold = importance_df['importance'].quantile(0.05)  # Bottom 5%
selected_features = importance_df[importance_df['importance'] > importance_threshold]['feature'].tolist()

print(f"\nFeature selection:")
print(f"  Original features: {len(all_feature_cols)}")
print(f"  After selection: {len(selected_features)}")
print(f"  Removed: {len(all_feature_cols) - len(selected_features)} low-importance features")

# ============================================================================
# 2.10: FINAL DATASET PREPARATION
# ============================================================================
print("\n" + "="*70)
print("FINAL DATASET PREPARATION")
print("="*70)

# Create final dataset with selected features
df_final = df_features[selected_features + ['target']].copy()

print(f"\nFinal dataset shape: {df_final.shape}")
print(f"  Samples: {df_final.shape[0]:,}")
print(f"  Features: {df_final.shape[1] - 1}")

print(f"\nTarget distribution:")
print(f"  Class 0 (Paid): {(df_final['target']==0).sum():,} ({(df_final['target']==0).mean()*100:.2f}%)")
print(f"  Class 1 (Default): {(df_final['target']==1).sum():,} ({(df_final['target']==1).mean()*100:.2f}%)")

print(f"\nData quality:")
print(f"  Missing values: {df_final.isnull().sum().sum()}")
print(f"  Duplicate rows: {df_final.duplicated().sum()}")

# ============================================================================
# 2.11: SAVE ENHANCED PREPROCESSED DATA
# ============================================================================
print("\n" + "="*70)
print("SAVING ENHANCED PREPROCESSED DATA")
print("="*70)

df_final.to_pickle('df_preprocessed.pkl')
print("âœ“ Saved: df_preprocessed.pkl")

# Save feature information
import pickle
feature_info = {
    'numerical_features': numerical_features,
    'categorical_features': categorical_features,
    'all_features': selected_features,
    'label_encoders': label_encoders,
    'importance_scores': importance_df.head(50).to_dict('records'),
    'feature_count': len(selected_features)
}

with open('feature_info.pkl', 'wb') as f:
    pickle.dump(feature_info, f)
print("âœ“ Saved: feature_info.pkl")

# ============================================================================
# 2.12: VISUALIZE IMPROVEMENTS
# ============================================================================
print("\n" + "="*70)
print("VISUALIZING FEATURE IMPROVEMENTS")
print("="*70)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Feature importance
ax1 = axes[0, 0]
top_20 = importance_df.head(20)
ax1.barh(range(20), top_20['importance'], color='steelblue')
ax1.set_yticks(range(20))
ax1.set_yticklabels([f[:25] for f in top_20['feature']], fontsize=8)
ax1.set_xlabel('Mutual Information Score')
ax1.set_title('Top 20 Most Important Features', fontweight='bold')
ax1.invert_yaxis()
ax1.grid(axis='x', alpha=0.3)

# 2. Target distribution
ax2 = axes[0, 1]
target_counts = df_final['target'].value_counts()
ax2.bar(['Paid', 'Default'], target_counts.values, color=['green', 'red'], alpha=0.7)
ax2.set_ylabel('Count')
ax2.set_title('Target Distribution', fontweight='bold')
ax2.grid(axis='y', alpha=0.3)
for i, v in enumerate(target_counts.values):
    ax2.text(i, v + 1000, f"{v:,}\n({v/len(df_final)*100:.1f}%)", 
            ha='center', fontweight='bold')

# 3. Feature type breakdown
ax3 = axes[1, 0]
feature_types = {
    'Original Numerical': len([f for f in selected_features if f in numerical_features]),
    'Original Categorical': len([f for f in selected_features if f in categorical_features]),
    'Interactions': len([f for f in selected_features if any(x in f for x in ['_to_', '_ratio', '_burden', 'combined'])]),
    'Binned': len([f for f in selected_features if 'bracket' in f]),
    'Missing Indicators': len([f for f in selected_features if 'is_missing' in f])
}
ax3.pie(feature_types.values(), labels=feature_types.keys(), autopct='%1.1f%%',
       colors=['blue', 'orange', 'green', 'red', 'purple'])
ax3.set_title('Feature Composition', fontweight='bold')

# 4. Sample correlations with target
ax4 = axes[1, 1]
top_features_for_corr = importance_df.head(15)['feature'].tolist()
correlations = []
for feat in top_features_for_corr:
    if feat in df_final.columns:
        corr = df_final[feat].corr(df_final['target'])
        correlations.append(abs(corr))
    else:
        correlations.append(0)

ax4.barh(range(len(top_features_for_corr)), correlations, color='coral')
ax4.set_yticks(range(len(top_features_for_corr)))
ax4.set_yticklabels([f[:25] for f in top_features_for_corr], fontsize=8)
ax4.set_xlabel('|Correlation| with Target')
ax4.set_title('Top 15 Features - Correlation Strength', fontweight='bold')
ax4.invert_yaxis()
ax4.grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.show()

print("âœ“ Visualizations complete!")

# ============================================================================
# 2.13: SUMMARY OF IMPROVEMENTS
# ============================================================================
print("\n" + "="*70)
print("SUMMARY OF ENHANCEMENTS")
print("="*70)

print("\nðŸŽ¯ KEY IMPROVEMENTS OVER ORIGINAL:")
print("\n1. MORE FEATURES")
print(f"   - Added delinq_2yrs, inq_last_6mths, collections (strong predictors)")
print(f"   - Added sub_grade for more granularity")
print(f"   - Total features: {len(selected_features)} (was ~19)")

print("\n2. MISSING VALUE INDICATORS")
print(f"   - Created is_missing flags (can be predictive)")
print(f"   - Class-specific median imputation (better than global median)")

print("\n3. POWERFUL INTERACTION FEATURES")
print(f"   - debt_to_income_ratio, payment_to_income, loan_to_income")
print(f"   - interest_burden, combined_risk_score")
print(f"   - high_utilization, has_bankruptcies flags")

print("\n4. BINNED FEATURES")
print(f"   - income_bracket, dti_bracket, rate_bracket")
print(f"   - Captures threshold effects (non-linear patterns)")

print("\n5. SMARTER OUTLIER HANDLING")
print(f"   - Percentile-based capping (99th) instead of hard thresholds")
print(f"   - Preserves more data while removing extremes")

print("\n6. FEATURE SELECTION")
print(f"   - Removed bottom 5% by mutual information")
print(f"   - Keeps only predictive features â†’ less noise")

print("\nðŸ“ˆ EXPECTED F1 IMPROVEMENT: +0.08 to +0.15")
print("   (Combined with aggressive training, target F1 > 0.55)")

print("\n" + "="*70)
print("ENHANCED STEP 2 COMPLETE!")
print("Next: Run Step 3 - Aggressive Deep Learning Training")
print("="*70)